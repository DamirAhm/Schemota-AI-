{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for AttributeError: module 'collections' has no attribute 'MutableMapping'\n",
    "import collections\n",
    "import collections.abc\n",
    "for type_name in collections.abc.__all__:\n",
    "    if not hasattr(collections, type_name):\n",
    "        setattr(collections, type_name, getattr(collections.abc, type_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "N = 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from catalyst import utils\n",
    "from catalyst.contrib.datasets import MNIST\n",
    "from torch import nn\n",
    "\n",
    "utils.set_global_seed(N)\n",
    "\n",
    "import six\n",
    "\n",
    "if not hasattr(six, 'string_classes'):\n",
    "    if hasattr(six, 'string_types'):\n",
    "        six.string_classes = six.string_types\n",
    "    else:\n",
    "        # For newer Python versions\n",
    "        six.string_classes = (str,)\n",
    "\n",
    "# Add this to ensure torch._six has string_classes\n",
    "import torch\n",
    "if not hasattr(torch, '_six'):\n",
    "    torch._six = six\n",
    "elif not hasattr(torch._six, 'string_classes'):\n",
    "    torch._six.string_classes = six.string_classes\n",
    "\n",
    "if not hasattr(torch, '_six'):\n",
    "    torch._six = six\n",
    "    \n",
    "import torch.nn.functional as F\n",
    "\n",
    "utils.set_global_seed(N)\n",
    "train_dataset = MNIST(root=os.getcwd(), train=True, download=True)\n",
    "val_dataset = MNIST(root=os.getcwd(), train=False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128)\n",
    "\n",
    "class Identical(nn.Module):\n",
    "\tdef forward(self, x):\n",
    "\t\treturn x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "\tdef forward(self, x):\n",
    "\t\tbatch_size = x.size(0)\n",
    "\t\t\n",
    "\t\treturn x.view(batch_size, -1)\n",
    "\n",
    "activation = Identical\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        return x.view(batch_size, -1)\n",
    "\n",
    "class EnhancedMNISTModel(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.2):\n",
    "        super(EnhancedMNISTModel, self).__init__()\n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc4 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the enhanced model\n",
    "model = EnhancedMNISTModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, 'max', patience=3, factor=0.5, verbose=True\n",
    ")\n",
    "\n",
    "loaders = {\"train\": train_dataloader, \"valid\": val_dataloader}\n",
    "\n",
    "max_epochs = N * 5\n",
    "\n",
    "accuracy = {\"train\": [], \"valid\": []}\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "\tepoch_correct = 0\n",
    "\tepoch_all = 0\n",
    "\tfor k, dataloader in loaders.items():\n",
    "\t\tepoch_correct = 0\n",
    "\t\tepoch_all = 0\n",
    "\t\tfor x_batch, y_batch in dataloader:\n",
    "\t\t\tif k == \"train\":\n",
    "\t\t\t\tmodel.train()\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\tx_batch = x_batch.float()\n",
    "\t\t\t\tx_batch = (x_batch - x_batch.mean()) / (x_batch.std() + 1e-8)\n",
    "\t\t\t\toutp = model(x_batch.float().unsqueeze(1))\n",
    "\t\t\telse:\n",
    "\t\t\t\tmodel.eval()\n",
    "\t\t\t\twith torch.no_grad():\n",
    "\t\t\t\t\tx_batch = x_batch.float()\n",
    "\t\t\t\t\tx_batch = (x_batch - x_batch.mean()) / (x_batch.std() + 1e-8)\n",
    "\t\t\t\t\toutp = model(x_batch.unsqueeze(1))\n",
    "\t\t\tpreds = outp.argmax(-1)\n",
    "\t\t\tcorrect = (preds == y_batch).sum()\n",
    "\t\t\tall = len(y_batch)\n",
    "\t\t\tepoch_correct += correct.item()\n",
    "\t\t\tepoch_all += all\n",
    "\t\t\tif k == \"train\":\n",
    "\t\t\t\tloss = criterion(outp, y_batch)\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\tprint(f\"Epoch: {epoch+1}\")\n",
    "\t\tprint(f\"Loader: {k}. Accuracy: {epoch_correct/epoch_all}\")\n",
    "\t\taccuracy[k].append(epoch_correct/epoch_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "epochs = list(range(1, max_epochs + 1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, accuracy['train'], 'b-', label='Training Accuracy')\n",
    "plt.plot(epochs, accuracy['valid'], 'r-', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
